{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "project_description",
      "metadata": {},
      "source": [
        "# RAG Project: Knowledge Retrieval from Local/Backend Data\n",
        "\n",
        "This notebook implements a Retrieval-Augmented Generation (RAG) pipeline to process locally stored articles (simulating a backend database) for accurate Q&A and summarization. The architecture uses **LangChain** for orchestration, **OpenAI** for embeddings and generation, and **FAISS** for vector storage.\n",
        "\n",
        "## \ud83d\udee0\ufe0f Setup Instructions\n",
        "1. **Install Dependencies:** `pip install langchain openai pydantic faiss-cpu python-dotenv`\n",
        "2. **Data Folder:** Create a folder named `articles/` in this notebook's directory and place your `.txt` articles inside it.\n",
        "3. **API Key:** Set your OpenAI API key in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c2aff87",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import langchain\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.schema import Document\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80fc5e57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set your OpenAI API key here.\n",
        "os.environ['OPENAI_API_KEY'] = 'your openapi key here'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39e721c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Large Language Model (LLM)\n",
        "llm = OpenAI(temperature=0.9, max_tokens=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0c3ff7",
      "metadata": {},
      "source": [
        "## (1) Data Ingestion: Fetching Articles from Simulated Backend (Local Files)\n",
        "\n",
        "This function replaces the original URL loader (`UnstructuredURLLoader`) by automatically loading all articles from the local `articles/` folder, simulating the text being pulled from your backend database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "data_loader_sim",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_articles_from_local(folder_path=\"articles/\"):\n",
        "    \"\"\"Loads all .txt files from a local folder, simulating a fetch from the backend.\"\"\"\n",
        "    try:\n",
        "        # DirectoryLoader is used to read multiple files from the local file system.\n",
        "        loader = DirectoryLoader(\n",
        "            folder_path, \n",
        "            glob=\"**/*.txt\", \n",
        "            loader_cls=TextLoader,\n",
        "            show_progress=True\n",
        "        )\n",
        "        data = loader.load()\n",
        "        print(f\"Successfully loaded {len(data)} documents from the '{folder_path}' folder.\")\n",
        "        # The metadata['source'] stores the file path, acting as the 'Article ID' reference.\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading local files. Please ensure the 'articles/' folder exists and contains .txt files. Error: {e}\")\n",
        "        return []\n",
        "        \n",
        "# --- Execute Data Loading ---\n",
        "data = load_articles_from_local()\n",
        "print(f\"Total documents loaded: {len(data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f51a5bd",
      "metadata": {},
      "source": [
        "## (2) Document Preprocessing: Splitting Articles into Chunks\n",
        "\n",
        "This step splits the loaded articles into smaller, overlapping chunks (the 'unit' of knowledge) suitable for embedding and LLM context limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "054a6361",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RecursiveCharacterTextSplitter is robust for complex text types.\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Split the loaded documents into chunks\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "print(f\"Total chunks created: {len(docs)}\")\n",
        "if docs:\n",
        "    print(\"\\nExample Chunk Source (Article ID):\", docs[0].metadata['source'])\n",
        "    print(\"Example Chunk Content (First 200 chars):\\n\", docs[0].page_content[:200] + '...') "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e35a876",
      "metadata": {},
      "source": [
        "## (3) Embedding and Vector Storage (FAISS)\n",
        "\n",
        "The chunks are converted to vector embeddings, which are then stored in FAISS (Facebook AI Similarity Search) for fast retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d0a6dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not docs:\n",
        "    raise ValueError(\"No documents were loaded or split. Cannot proceed with embedding.\")\n",
        "    \n",
        "# Create the embeddings of the chunks\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Create FAISS vector index from documents and embeddings\n",
        "print(\"Creating FAISS Vector Index... (This might take a moment)\")\n",
        "vectorindex_openai = FAISS.from_documents(docs, embeddings)\n",
        "print(\"FAISS Vector Index creation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9686c13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Storing vector index to local disk for persistence (saving to 'vector_index.pkl')\n",
        "file_path=\"vector_index.pkl\"\n",
        "with open(file_path, \"wb\") as f:\n",
        "    pickle.dump(vectorindex_openai, f)\n",
        "print(f\"Vector Index saved to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "688dc29b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading vector index from local disk (for subsequent runs)\n",
        "file_path=\"vector_index.pkl\"\n",
        "if os.path.exists(file_path):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        vectorIndex = pickle.load(f)\n",
        "    print(\"Vector Index loaded successfully from local file.\")\n",
        "else:\n",
        "    print(\"Vector Index file not found. Please run the previous cells to create it.\")\n",
        "    vectorIndex = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd96296",
      "metadata": {},
      "source": [
        "## (4) Retrieval-Augmented Generation (RAG) and Q&A\n",
        "\n",
        "The chain takes a user query, finds relevant chunks using the FAISS index, and passes both to the LLM to generate a final, sourced answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01f5e1e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the RetrievalQA chain\n",
        "if vectorIndex is not None:\n",
        "    chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=vectorIndex.as_retriever())\n",
        "    print(\"RetrievalQA Chain initialized.\")\n",
        "else:\n",
        "    print(\"Cannot initialize chain: Vector Index is missing.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c2e228b",
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Set your question (Query) based on the articles you placed in the 'articles/' folder\n",
        "query = \"What are the key points of the latest technology policy article?\"\n",
        "\n",
        "print(f\"Question: {query}\\n\")\n",
        "\n",
        "# Set LangChain to debug mode to see the internal steps (optional)\n",
        "langchain.debug=True\n",
        "\n",
        "# Run the RAG Chain\n",
        "if 'chain' in locals():\n",
        "    # Note: 'return_only_outputs=True' returns a dictionary with 'answer' and 'sources'\n",
        "    result = chain({\"question\": query}, return_only_outputs=True)\n",
        "    \n",
        "    # Print the final result in a clean format\n",
        "    print(\"\\n--- RAG RESULT ---\")\n",
        "    print(\"Answer:\", result['answer'].strip())\n",
        "    print(\"Sources:\", result['sources'])\n",
        "else:\n",
        "    print(\"RAG Chain not ready. Please check the previous steps.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}